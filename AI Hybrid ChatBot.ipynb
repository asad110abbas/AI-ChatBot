{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1fgfEyqDju15b0-AmbvIi58ZTDORg6u5j","authorship_tag":"ABX9TyOiFG0VG5NJGeaL1sHW8+rm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install transformers torch accelerate bitsandbytes sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhlUwvj9ys34","executionInfo":{"status":"ok","timestamp":1741083897477,"user_tz":-300,"elapsed":3739,"user":{"displayName":"Asad Abbas","userId":"11707438799168579931"}},"outputId":"061a61e7-7cac-4d89-a157-55c1e386343b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}]},{"cell_type":"code","source":["pip install llama-cpp-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7vyXf_kytWF","executionInfo":{"status":"ok","timestamp":1741084135743,"user_tz":-300,"elapsed":228621,"user":{"displayName":"Asad Abbas","userId":"11707438799168579931"}},"outputId":"db04a5c5-01af-404c-903e-4d3d5b0b37ba"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp311-cp311-linux_x86_64.whl size=4552823 sha256=700a3385eb5dbd11a40e6323593aed87f7f291220502db49b2c82b00d089243e\n","  Stored in directory: /root/.cache/pip/wheels/eb/82/79/ac77fcd49324b75ae6aa18e63a87cf9da4371a57e2cdc8dc03\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n"]}]},{"cell_type":"markdown","source":["# **Move Model in a Colab Space**"],"metadata":{"id":"ukkp12P1Gf97"}},{"cell_type":"code","source":["import shutil\n","\n","# ‚úÖ Model ko Google Drive se Local Colab Storage Pe Copy Karo\n","model_source = \"/content/drive/MyDrive/Llama Model/llama-2-7b.Q4_K_M.gguf\"\n","model_dest = \"/content/llama-2-7b.Q4_K_M.gguf\"\n","\n","shutil.copy(model_source, model_dest)\n","print(\"‚úÖ Model copied to local storage!\")\n","\n","# ‚úÖ Ab local path se load karo\n","model_path = \"/content/llama-2-7b.Q4_K_M.gguf\"\n","llm = Llama(model_path=model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"dVJwfgQoFJXS","executionInfo":{"status":"error","timestamp":1741084258559,"user_tz":-300,"elapsed":75136,"user":{"displayName":"Asad Abbas","userId":"11707438799168579931"}},"outputId":"7f412e7e-cda1-430a-be4c-65c032852eb8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Model copied to local storage!\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'Llama' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-38619a20566a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# ‚úÖ Ab local path se load karo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/llama-2-7b.Q4_K_M.gguf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'Llama' is not defined"]}]},{"cell_type":"markdown","source":["# **Embedding a Data**"],"metadata":{"id":"Y_O8FaTOGoEo"}},{"cell_type":"code","source":["import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","\n","# Initialize model\n","embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n","\n","# Load data\n","df = pd.read_csv(\"product-data.csv\")\n","\n","# Create combined features\n","df[\"features\"] = \"Color: \" + df[\"color\"] + \", Memory: \" + df[\"memory\"]\n","\n","# Create description with PKR formatting\n","df[\"description\"] = (\n","    df[\"product_title\"] + \" | \" +\n","    \"Brand: \" + df[\"brand\"] + \" | \" +\n","    \"Price: PKR \" + df[\"price\"].astype(str) + \" | \" +\n","    \"Features: \" + df[\"features\"]\n",")\n","\n","# Generate embeddings\n","df[\"embedding\"] = df[\"description\"].apply(lambda x: embedder.encode(x).tolist())\n","\n","# Save with required columns\n","df[['product_title', 'brand', 'price', 'features', 'embedding']].to_pickle(\"product_embeddings.pkl\")\n","print(\"‚úÖ Embeddings generated with Pakistan-specific formatting!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dG3Op3uzXz5","executionInfo":{"status":"ok","timestamp":1741085546394,"user_tz":-300,"elapsed":941664,"user":{"displayName":"Asad Abbas","userId":"11707438799168579931"}},"outputId":"faec2fa5-6b55-44f5-a030-70fcbf663e3b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Embeddings generated with Pakistan-specific formatting!\n"]}]},{"cell_type":"markdown","source":["# **Model Creation**"],"metadata":{"id":"ExVr01gZJWi2"}},{"cell_type":"code","source":["\\import os\n","import pandas as pd\n","import pickle\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from llama_cpp import Llama\n","\n","# ----- Configuration & File Paths -----\n","CSV_FILE = \"product-data.csv\" # CSV file containing product data\n","EMBEDDING_FILE = \"product_embeddings.pkl\" # File to cache computed embeddings\n","DEBUG_MODE = False # Set True for debugging\n","\n","# Load a robust embedding model\n","embedding_model_name = \"all-mpnet-base-v2\"\n","embedder = SentenceTransformer(embedding_model_name)\n","\n","# Path to your LLaMA model (update this path accordingly)\n","llama_model_path = \"/content/llama-2-7b.Q4_K_M.gguf\"\n","llama = Llama(model_path=llama_model_path, n_ctx=4096)\n","\n","# ----- Load Product Data and Embeddings -----\n","if os.path.exists(EMBEDDING_FILE):\n","    with open(EMBEDDING_FILE, \"rb\") as f:\n","        df = pickle.load(f)\n","    if DEBUG_MODE:\n","        print(\"‚úÖ Loaded precomputed embeddings from pickle.\")\n","else:\n","    df = pd.read_csv(CSV_FILE)\n","    # Create a description column if not already present\n","    df[\"description\"] = (\n","        df[\"product_title\"] + \" - \" + df[\"brand\"] + \" - \" +\n","        df[\"color\"] + \" - \" + df[\"memory\"] + \" - Rs.\" + df[\"price\"].astype(str)\n","    )\n","    df[\"embedding\"] = df[\"description\"].apply(lambda text: embedder.encode(text).tolist())\n","    with open(EMBEDDING_FILE, \"wb\") as f:\n","        pickle.dump(df, f)\n","    if DEBUG_MODE:\n","        print(\"‚úÖ Embeddings computed and saved.\")\n","\n","# ----- Product Recommendation Function -----\n","def recommend_product(query: str, top_k: int = 1) -> str:\n","    \"\"\"\n","    Uses precomputed embeddings and the LLaMA model to provide a single, concise, final product\n","    recommendation in a friendly, ChatGPT-like tone based solely on CSV product data.\n","    \"\"\"\n","    # Compute the embedding for the user query\n","    query_emb = embedder.encode(query)\n","\n","    # Calculate cosine similarity for each product and select the best match.\n","    df[\"similarity\"] = df[\"embedding\"].apply(lambda emb: cosine_similarity([emb], [query_emb])[0][0])\n","    df_sorted = df.sort_values(\"similarity\", ascending=False)\n","    top_products = df_sorted.head(top_k)\n","\n","    # Build product details string for the best match.\n","    if top_products.iloc[0][\"similarity\"] < 0.2:\n","        product_details = \"No matching products found in our catalog.\"\n","    else:\n","        # Since top_k is 1, loop should run only once.\n","        product_details = \"\"\n","        for idx, row in top_products.iterrows():\n","            product_details = (\n","                f\"Product Name: {row['product_title']}\\n\"\n","                f\"Brand: {row['brand']}\\n\"\n","                f\"Color: {row.get('color', 'N/A')}\\n\"\n","                f\"Memory: {row.get('memory', 'N/A')}\\n\"\n","                f\"Price: {row.get('price', 'N/A')}\\n\"\n","            )\n","\n","    # Construct a prompt with very explicit instructions.\n","    prompt = f\"\"\"\n","You are a friendly and knowledgeable product recommendation assistant.\n","Based ONLY on the product data provided below (from a CSV file), answer the user's query in a natural, conversational, and precise manner.\n","Do NOT include any of the context, headers, or product data in your output.if user ask for a multiple products\n","Provide ONLY one final answer exactly in the format specified below.use a conversational tone and answer the uqeries accordingly which are related to products.\n","\n","Final Answer Format:\n","Product Name: [Name]\n","Brand: [Brand]\n","Color: [Color]\n","Memory: [Memory]\n","Price: [Price]\n","\n","User Query: \"{query}\"\n","Product Data:\n","{product_details}\n","\n","Now, provide ONLY your final answer below (do not repeat any above text):\n","\"\"\"\n","\n","    if DEBUG_MODE:\n","        print(\"DEBUG - Prompt for LLaMA:\")\n","        print(prompt)\n","\n","    # Generate the recommendation using LLaMA.\n","    response = llama(\n","        prompt,\n","        max_tokens=500,      # Increase max_tokens if needed\n","        temperature=0.4,     # Lower temperature for a more deterministic response\n","        stop=[\"\\n\\n\"]       # Use a stop token to cut off extra output\n","    )\n","\n","    if DEBUG_MODE:\n","        print(\"DEBUG - Raw LLaMA Response:\")\n","        print(response)\n","\n","    # Extract and return the generated text.\n","    if \"choices\" in response and len(response[\"choices\"]) > 0:\n","        recommendation = response[\"choices\"][0][\"text\"].strip()\n","    else:\n","        recommendation = \"‚ùå No recommendation generated.\"\n","\n","    return recommendation\n","\n","# ----- Main Loop -----\n","if __name__ == \"__main__\":\n","    print(\"Product Recommendation Bot (type 'exit' to quit)\")\n","    while True:\n","        user_query = input(\"\\nüîç Enter your product query: \")\n","        if user_query.lower() == \"exit\":\n","            print(\"üëã Exiting...\")\n","            break\n","        rec = recommend_product(user_query)\n","        print(\"\\nü§ñ AI Recommendation:\\n\")\n","        print(rec)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ry_nam9oFU4S","outputId":"7250e10a-77d6-45c9-83ca-fe57d0950126","executionInfo":{"status":"error","timestamp":1741088777046,"user_tz":-300,"elapsed":2838680,"user":{"displayName":"Asad Abbas","userId":"11707438799168579931"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /content/llama-2-7b.Q4_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 15\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","print_info: file format = GGUF V2\n","print_info: file type   = Q4_K - Medium\n","print_info: file size   = 3.80 GiB (4.84 BPW) \n","init_tokenizer: initializing tokenizer for type 1\n","load: control token:      2 '</s>' is not marked as EOG\n","load: control token:      1 '<s>' is not marked as EOG\n","load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","load: special tokens cache size = 3\n","load: token to piece cache size = 0.1684 MB\n","print_info: arch             = llama\n","print_info: vocab_only       = 0\n","print_info: n_ctx_train      = 4096\n","print_info: n_embd           = 4096\n","print_info: n_layer          = 32\n","print_info: n_head           = 32\n","print_info: n_head_kv        = 32\n","print_info: n_rot            = 128\n","print_info: n_swa            = 0\n","print_info: n_embd_head_k    = 128\n","print_info: n_embd_head_v    = 128\n","print_info: n_gqa            = 1\n","print_info: n_embd_k_gqa     = 4096\n","print_info: n_embd_v_gqa     = 4096\n","print_info: f_norm_eps       = 0.0e+00\n","print_info: f_norm_rms_eps   = 1.0e-05\n","print_info: f_clamp_kqv      = 0.0e+00\n","print_info: f_max_alibi_bias = 0.0e+00\n","print_info: f_logit_scale    = 0.0e+00\n","print_info: n_ff             = 11008\n","print_info: n_expert         = 0\n","print_info: n_expert_used    = 0\n","print_info: causal attn      = 1\n","print_info: pooling type     = 0\n","print_info: rope type        = 0\n","print_info: rope scaling     = linear\n","print_info: freq_base_train  = 10000.0\n","print_info: freq_scale_train = 1\n","print_info: n_ctx_orig_yarn  = 4096\n","print_info: rope_finetuned   = unknown\n","print_info: ssm_d_conv       = 0\n","print_info: ssm_d_inner      = 0\n","print_info: ssm_d_state      = 0\n","print_info: ssm_dt_rank      = 0\n","print_info: ssm_dt_b_c_rms   = 0\n","print_info: model type       = 7B\n","print_info: model params     = 6.74 B\n","print_info: general.name     = LLaMA v2\n","print_info: vocab type       = SPM\n","print_info: n_vocab          = 32000\n","print_info: n_merges         = 0\n","print_info: BOS token        = 1 '<s>'\n","print_info: EOS token        = 2 '</s>'\n","print_info: UNK token        = 0 '<unk>'\n","print_info: LF token         = 13 '<0x0A>'\n","print_info: EOG token        = 2 '</s>'\n","print_info: max token length = 48\n","load_tensors: layer   0 assigned to device CPU\n","load_tensors: layer   1 assigned to device CPU\n","load_tensors: layer   2 assigned to device CPU\n","load_tensors: layer   3 assigned to device CPU\n","load_tensors: layer   4 assigned to device CPU\n","load_tensors: layer   5 assigned to device CPU\n","load_tensors: layer   6 assigned to device CPU\n","load_tensors: layer   7 assigned to device CPU\n","load_tensors: layer   8 assigned to device CPU\n","load_tensors: layer   9 assigned to device CPU\n","load_tensors: layer  10 assigned to device CPU\n","load_tensors: layer  11 assigned to device CPU\n","load_tensors: layer  12 assigned to device CPU\n","load_tensors: layer  13 assigned to device CPU\n","load_tensors: layer  14 assigned to device CPU\n","load_tensors: layer  15 assigned to device CPU\n","load_tensors: layer  16 assigned to device CPU\n","load_tensors: layer  17 assigned to device CPU\n","load_tensors: layer  18 assigned to device CPU\n","load_tensors: layer  19 assigned to device CPU\n","load_tensors: layer  20 assigned to device CPU\n","load_tensors: layer  21 assigned to device CPU\n","load_tensors: layer  22 assigned to device CPU\n","load_tensors: layer  23 assigned to device CPU\n","load_tensors: layer  24 assigned to device CPU\n","load_tensors: layer  25 assigned to device CPU\n","load_tensors: layer  26 assigned to device CPU\n","load_tensors: layer  27 assigned to device CPU\n","load_tensors: layer  28 assigned to device CPU\n","load_tensors: layer  29 assigned to device CPU\n","load_tensors: layer  30 assigned to device CPU\n","load_tensors: layer  31 assigned to device CPU\n","load_tensors: layer  32 assigned to device CPU\n","load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n","load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\n","llama_init_from_model: n_seq_max     = 1\n","llama_init_from_model: n_ctx         = 4096\n","llama_init_from_model: n_ctx_per_seq = 4096\n","llama_init_from_model: n_batch       = 512\n","llama_init_from_model: n_ubatch      = 512\n","llama_init_from_model: flash_attn    = 0\n","llama_init_from_model: freq_base     = 10000.0\n","llama_init_from_model: freq_scale    = 1\n","llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n","llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n","llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n","llama_init_from_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n","llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n","llama_init_from_model:        CPU compute buffer size =   296.01 MiB\n","llama_init_from_model: graph nodes  = 1030\n","llama_init_from_model: graph splits = 1\n","CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n","Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n","Using fallback chat format: llama-2\n"]},{"name":"stdout","output_type":"stream","text":["Product Recommendation Bot (type 'exit' to quit)\n","\n","üîç Enter your product query: Samsung Galaxy A06\n"]},{"output_type":"stream","name":"stderr","text":["llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   89178.60 ms /   223 tokens (  399.90 ms per token,     2.50 tokens per second)\n","llama_perf_context_print:        eval time =   27097.98 ms /    39 runs   (  694.82 ms per token,     1.44 tokens per second)\n","llama_perf_context_print:       total time =  116299.51 ms /   262 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Samsung Galaxy A06\n","Brand: Samsung\n","Color: N/A\n","Memory: N/A\n","Price: 31500\n","\n","üîç Enter your product query: Samsung Galaxy A05s\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 158 prefix-match hit, remaining 67 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   26478.62 ms /    67 tokens (  395.20 ms per token,     2.53 tokens per second)\n","llama_perf_context_print:        eval time =   31455.34 ms /    44 runs   (  714.89 ms per token,     1.40 tokens per second)\n","llama_perf_context_print:       total time =   57960.25 ms /   111 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Final Answer:\n","Product Name: Samsung Galaxy A05s\n","Brand: Samsung\n","Color: N/A\n","Memory: N/A\n","Price: 32300\n","\n","üîç Enter your product query: Samsung Galaxy A04s\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 158 prefix-match hit, remaining 67 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   27744.98 ms /    67 tokens (  414.10 ms per token,     2.41 tokens per second)\n","llama_perf_context_print:        eval time =   27664.41 ms /    39 runs   (  709.34 ms per token,     1.41 tokens per second)\n","llama_perf_context_print:       total time =   55434.87 ms /   106 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Samsung Galaxy A04s\n","Brand: Samsung\n","Color: N/A\n","Memory: N/A\n","Price: 27999\n","\n","üîç Enter your product query: Xiaomi Redmi A3x\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 152 prefix-match hit, remaining 74 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   29537.42 ms /    74 tokens (  399.15 ms per token,     2.51 tokens per second)\n","llama_perf_context_print:        eval time =   28208.21 ms /    40 runs   (  705.21 ms per token,     1.42 tokens per second)\n","llama_perf_context_print:       total time =   57768.86 ms /   114 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Xiaomi Redmi A3x\n","Brand: Xiaomi\n","Color: N/A\n","Memory: N/A\n","Price: 16499\n","\n","üîç Enter your product query: Xiaomi Redmi A3x \n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 160 prefix-match hit, remaining 66 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   29382.06 ms /    66 tokens (  445.18 ms per token,     2.25 tokens per second)\n","llama_perf_context_print:        eval time =   28016.03 ms /    40 runs   (  700.40 ms per token,     1.43 tokens per second)\n","llama_perf_context_print:       total time =   57421.99 ms /   106 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Xiaomi Redmi A3x\n","Brand: Xiaomi\n","Color: N/A\n","Memory: N/A\n","Price: 16499\n","\n","üîç Enter your product query: Xiaomi Redmi A3x\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 160 prefix-match hit, remaining 66 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   26888.86 ms /    66 tokens (  407.41 ms per token,     2.45 tokens per second)\n","llama_perf_context_print:        eval time =   28083.90 ms /    40 runs   (  702.10 ms per token,     1.42 tokens per second)\n","llama_perf_context_print:       total time =   54996.16 ms /   106 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Xiaomi Redmi A3x\n","Brand: Xiaomi\n","Color: N/A\n","Memory: N/A\n","Price: 16499\n","\n","üîç Enter your product query: \n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 151 prefix-match hit, remaining 34 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   17301.54 ms /    34 tokens (  508.87 ms per token,     1.97 tokens per second)\n","llama_perf_context_print:        eval time =   22504.89 ms /    33 runs   (  681.97 ms per token,     1.47 tokens per second)\n","llama_perf_context_print:       total time =   39825.16 ms /    67 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: [Name]\n","Brand: [Brand]\n","Color: [Color]\n","Memory: [Memory]\n","Price: [Price]\n","\n","üîç Enter your product query: Samsung Galaxy A06\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 151 prefix-match hit, remaining 72 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   37435.63 ms /    72 tokens (  519.94 ms per token,     1.92 tokens per second)\n","llama_perf_context_print:        eval time =   27349.09 ms /    39 runs   (  701.26 ms per token,     1.43 tokens per second)\n","llama_perf_context_print:       total time =   64807.73 ms /   111 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Samsung Galaxy A06\n","Brand: Samsung\n","Color: N/A\n","Memory: N/A\n","Price: 31500\n","\n","üîç Enter your product query: xiaomi A3x\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 152 prefix-match hit, remaining 72 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   29174.24 ms /    72 tokens (  405.20 ms per token,     2.47 tokens per second)\n","llama_perf_context_print:        eval time =   28576.42 ms /    41 runs   (  696.99 ms per token,     1.43 tokens per second)\n","llama_perf_context_print:       total time =   57773.65 ms /   113 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Xiaomi Redmi A3x\n","Brand: Xiaomi\n","Color: N/A\n","Memory: N/A\n","Price: 16499\n","\n","üîç Enter your product query: Samsung Galaxy A06\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 152 prefix-match hit, remaining 71 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   30816.93 ms /    71 tokens (  434.04 ms per token,     2.30 tokens per second)\n","llama_perf_context_print:        eval time =   26801.12 ms /    38 runs   (  705.29 ms per token,     1.42 tokens per second)\n","llama_perf_context_print:       total time =   57640.44 ms /   109 tokens\n"]},{"name":"stdout","output_type":"stream","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Samsung Galaxy A06\n","Brand: Samsung\n","Color: N/A\n","Memory: N/A\n","Price: 31500\n","\n","üîç Enter your product query: Vivo Y33\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: 152 prefix-match hit, remaining 67 prompt tokens to eval\n","llama_perf_context_print:        load time =   89179.42 ms\n","llama_perf_context_print: prompt eval time =   29290.19 ms /    67 tokens (  437.17 ms per token,     2.29 tokens per second)\n","llama_perf_context_print:        eval time =   26060.92 ms /    37 runs   (  704.35 ms per token,     1.42 tokens per second)\n","llama_perf_context_print:       total time =   55372.49 ms /   104 tokens\n"]},{"output_type":"stream","name":"stdout","text":["\n","ü§ñ AI Recommendation:\n","\n","Product Name: Vivo Y28\n","Brand: Vivo\n","Color: N/A\n","Memory: N/A\n","Price: 40499\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-9a79eab3ff8c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Product Recommendation Bot (type 'exit' to quit)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüîç Enter your product query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üëã Exiting...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"markdown","source":["# **Satisfactory**"],"metadata":{"id":"BZ09jteJCZPi"}},{"cell_type":"code","source":["import os\n","import re\n","import pandas as pd\n","import pickle\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from llama_cpp import Llama\n","\n","# ----- Configuration -----\n","CSV_FILE = \"product-data.csv\"\n","EMBEDDING_FILE = \"product_embeddings.pkl\"\n","SIMILARITY_THRESHOLD = 0.55\n","PKR_RANGE_BUFFER = 5000\n","BRANDS = ['xiaomi', 'samsung', 'infinix', 'realme', 'oppo', 'vivo', 'tecno', 'nokia']\n","\n","# Initialize models\n","embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n","llama = Llama(\n","    model_path=\"/content/llama-2-7b.Q4_K_M.gguf\",\n","    n_ctx=2048,\n","    n_threads=4,\n","    chat_format=\"chatml\",\n","    verbose=False\n",")\n","\n","# ----- Data Loading -----\n","def load_product_data():\n","    if os.path.exists(EMBEDDING_FILE):\n","        with open(EMBEDDING_FILE, \"rb\") as f:\n","            df = pickle.load(f)\n","            required_cols = ['product_title', 'brand', 'price', 'features', 'embedding']\n","            if not all(col in df.columns for col in required_cols):\n","                raise ValueError(\"Corrupt embeddings file. Please delete and regenerate.\")\n","            return df\n","\n","    # Load and process CSV\n","    df = pd.read_csv(CSV_FILE)\n","    required_cols = ['product_title', 'brand', 'price', 'color', 'memory']\n","    if not all(col in df.columns for col in required_cols):\n","        missing = [col for col in required_cols if col not in df.columns]\n","        raise ValueError(f\"Missing columns: {', '.join(missing)}\")\n","\n","    # Create features\n","    df[\"features\"] = \"Color: \" + df[\"color\"].str.title() + \", Memory: \" + df[\"memory\"].str.upper()\n","    df[\"brand\"] = df[\"brand\"].str.lower()\n","\n","    # Generate embeddings\n","    print(\"üî® Generating Pakistan-market embeddings...\")\n","    descriptions = [\n","        f\"{row['product_title']} | Brand: {row['brand']} | Price: PKR {row['price']} | {row['features']}\"\n","        for _, row in df.iterrows()\n","    ]\n","    df[\"embedding\"] = embedder.encode(descriptions, batch_size=32, show_progress_bar=True).tolist()\n","\n","    # Save for future\n","    df[['product_title', 'brand', 'price', 'features', 'embedding']].to_pickle(EMBEDDING_FILE)\n","    return df\n","# ----- Price Extraction -----\n","def extract_price_range(query: str) -> tuple:\n","    \"\"\"Improved price parser for Pakistani formats\"\"\"\n","    # Corrected regex pattern\n","    query = re.sub(r'[^\\d.]', '', query.lower().replace('rs', '').replace('‚Çπ', '').replace(',', ''))\n","\n","    # Find all price mentions\n","    numbers = [float(match) for match in re.findall(r'\\d+\\.?\\d*', query)]\n","\n","    # Handle conversion terms\n","    conversions = {\n","        'k': 1000,\n","        'thousand': 1000,\n","        'lakh': 100000,\n","        'hazar': 1000\n","    }\n","    for term, multiplier in conversions.items():\n","        if term in query:\n","            numbers = [n * multiplier for n in numbers]\n","\n","    if not numbers:\n","        return (0, float('inf'))\n","\n","    # Determine range logic\n","    if 'under' in query or 'below' in query:\n","        return (0, numbers[0] + PKR_RANGE_BUFFER)\n","    elif 'above' in query or 'over' in query:\n","        return (max(0, numbers[0] - PKR_RANGE_BUFFER), float('inf'))  # Fixed line\n","    elif len(numbers) >= 2:\n","        return (min(numbers), max(numbers) + PKR_RANGE_BUFFER)\n","    else:\n","        return (max(0, numbers[0] - PKR_RANGE_BUFFER), numbers[0] + PKR_RANGE_BUFFER)\n","# ----- Recommendation Engine -----\n","def recommend_product(query: str, df: pd.DataFrame) -> str:\n","    # Detect mentioned brands\n","    query_lower = query.lower()\n","    mentioned_brands = [brand for brand in BRANDS if brand in query_lower]\n","\n","    # Filter by brand if mentioned\n","    if mentioned_brands:\n","        brand_filter = df['brand'].isin(mentioned_brands)\n","        df = df[brand_filter]\n","        if df.empty:\n","            return f\"‚ùå No {mentioned_brands[0].title()} phones found. Try other brands.\"\n","\n","    # Extract price range\n","    min_price, max_price = extract_price_range(query)\n","    filtered_df = df[(df['price'] >= min_price) & (df['price'] <= max_price)]\n","\n","    if filtered_df.empty:\n","        return f\"ü§∑ No phones in PKR {min_price:,}-{max_price:,}. Try wider range.\"\n","\n","    # Calculate similarities\n","    query_emb = embedder.encode(query)\n","    filtered_df[\"similarity\"] = cosine_similarity([query_emb], filtered_df[\"embedding\"].tolist())[0]\n","    top_products = filtered_df.nlargest(3, \"similarity\")\n","\n","    if top_products.iloc[0][\"similarity\"] < SIMILARITY_THRESHOLD:\n","        return (\"üîç Be specific:\\n\"\n","                f\"- Price range (Current: PKR {min_price:,}-{max_price:,})\\n\"\n","                \"- Brand preference\\n\"\n","                \"- Features needed\")\n","\n","    # Prepare product info\n","    products_info = \"\\n\\n\".join([\n","        f\"üì¶ {row['product_title']}\\n\"\n","        f\"üè∑Ô∏è Brand: {row['brand'].title()}\\n\"\n","        f\"üíµ Price: PKR {row['price']:,}\\n\"\n","        f\"üöÄ Features: {row['features']}\"\n","        for _, row in top_products.iterrows()\n","    ])\n","\n","    # LLM Prompt with brand context\n","    brand_context = f\"focusing on {mentioned_brands[0].title()}\" if mentioned_brands else \"considering all brands\"\n","    prompt = f\"\"\"<|im_start|>system\n","You're a smartphone expert {brand_context} in Pakistan. Rules:\n","1. Use ONLY these products:\n","{products_info}\n","2. Highlight features matching: {query}\n","3. Compare prices clearly in PKR\n","4. Use emoji bullet points (‚úîÔ∏è, üî•, üí°)\n","5. Keep under 150 words\n","6. Never mention unavailable products\n","\n","Example Response:\n","\"Top Xiaomi phones under PKR 50k:\n","\n","üì± Redmi Note 12 Pro (PKR 49,999)\n","‚úîÔ∏è 120Hz AMOLED Display\n","‚úîÔ∏è 108MP OIS Camera\n","üí° Best camera in range\n","\n","üì± Poco X5 Pro (PKR 47,499)\n","‚úîÔ∏è Snapdragon 778G\n","üî• Top gaming performance\n","\n","Need detailed specs?\"<|im_end|>\n","<|im_start|>user\n","{query}<|im_end|>\n","<|im_start|>assistant\n","üáµüá∞ {mentioned_brands[0].title() + ' Expert' if mentioned_brands else 'Phone Finder'}:\n","\"\"\"\n","\n","    # Generate response\n","    response = llama(\n","        prompt,\n","        max_tokens=350,\n","        temperature=0.7,\n","        top_p=0.9,\n","        stop=[\"<|im_end|>\"],\n","        echo=False\n","    )\n","\n","    return response[\"choices\"][0][\"text\"].strip()\n","\n","# ----- User Interface -----\n","if __name__ == \"__main__\":\n","    try:\n","        df = load_product_data()\n","        print(\"\\nüåü Welcome to Pakistani Phone Finder! Ask like 'Best Xiaomi under 50k?'\\n\")\n","\n","        while True:\n","            try:\n","                query = input(\"You: \").strip()\n","                if not query:\n","                    continue\n","                if query.lower() in ('exit', 'quit', 'bye'):\n","                    print(\"\\nAssistant: Shukriya! Visit us for great deals! üõçÔ∏è\")\n","                    break\n","\n","                print(\"\\nAssistant: \", end=\"\", flush=True)\n","                response = recommend_product(query, df)\n","                print(response + \"\\n\")\n","\n","            except KeyboardInterrupt:\n","                print(\"\\nAssistant: Session saved. Allah Hafiz! üëã\")\n","                break\n","\n","    except Exception as e:\n","        print(f\"\\n‚ö†Ô∏è Error: {str(e)}\\nPlease check your data and try again.\")"],"metadata":{"id":"PtSG5bCEB6gt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740746396994,"user_tz":-300,"elapsed":30721,"user":{"displayName":"Asad Abbas","userId":"11707438799168579931"}},"outputId":"b9370d99-3937-4b7f-a5e1-c429df4ae460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"]},{"output_type":"stream","name":"stdout","text":["\n","üåü Welcome to Pakistani Phone Finder! Ask like 'Best Xiaomi under 50k?'\n","\n","You: best phone under 60000rs\n","\n","Assistant: "]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-b2f2a0864d7f>:112: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_df[\"similarity\"] = cosine_similarity([query_emb], filtered_df[\"embedding\"].tolist())[0]\n"]},{"output_type":"stream","name":"stdout","text":["üîç Be specific:\n","- Price range (Current: PKR 55,000.0-65,000.0)\n","- Brand preference\n","- Features needed\n","\n","\n","Assistant: Session saved. Allah Hafiz! üëã\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jMS0HvuzDPEC"},"execution_count":null,"outputs":[]}]}